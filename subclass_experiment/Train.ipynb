{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import packages \n",
    "\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "folder_path=\"./dataset/subset/\"\n",
    "folders = glob(folder_path+'complete/*')\n",
    "\n",
    "train_split=0.7\n",
    "no_of_images=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef createFolder(path):\\n\\tif os.path.exists(path):\\n\\t\\tshutil.rmtree(path)\\n\\tos.makedirs(path)\\n\\ncreateFolder(folder_path+\"train\")\\ncreateFolder(folder_path+\"test\")\\n\\nfor folder in folders:\\n    images = glob(folder+\\'/*.jpg\\')\\n    name = folder.split(\\'/\\')[-1]\\n    createFolder(folder_path+\"train/\"+name)\\n    createFolder(folder_path+\"test/\"+name)\\n    train_images=math.floor(train_split*len(images))\\n    for i,image in enumerate(images):\\n        if(i<=train_images-1):\\n            shutil.copy(image,folder_path+\"train/\"+name)\\n        else:\\n            shutil.copy(image,folder_path+\"test/\"+name)\\n        no_of_images+=1\\n    print(name)\\n'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split data to training and testing\n",
    "'''\n",
    "def createFolder(path):\n",
    "\tif os.path.exists(path):\n",
    "\t\tshutil.rmtree(path)\n",
    "\tos.makedirs(path)\n",
    "\n",
    "createFolder(folder_path+\"train\")\n",
    "createFolder(folder_path+\"test\")\n",
    "\n",
    "for folder in folders:\n",
    "    images = glob(folder+'/*.jpg')\n",
    "    name = folder.split('/')[-1]\n",
    "    createFolder(folder_path+\"train/\"+name)\n",
    "    createFolder(folder_path+\"test/\"+name)\n",
    "    train_images=math.floor(train_split*len(images))\n",
    "    for i,image in enumerate(images):\n",
    "        if(i<=train_images-1):\n",
    "            shutil.copy(image,folder_path+\"train/\"+name)\n",
    "        else:\n",
    "            shutil.copy(image,folder_path+\"test/\"+name)\n",
    "        no_of_images+=1\n",
    "    print(name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8751 images belonging to 5 classes.\n",
      "Found 3753 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#generate train and test data\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   horizontal_flip=True,\n",
    "                                   rotation_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(folder_path+\"train/\",\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(folder_path+\"test/\",\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 813,733\n",
      "Trainable params: 813,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential,save_model,load_model\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.applications import VGG16,Xception,ResNet50,nasnet\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping, TensorBoard\n",
    "from keras.optimizers import SGD,rmsprop,Adam\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Dropout(0.2))\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Dropout(0.2))\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 5, activation = 'softmax'))\n",
    "# Compiling the CNN\n",
    "# opt = rmsprop(lr=0.0001, decay=1e-6)\n",
    "# opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "classifier.compile(optimizer = 'adam',\n",
    "                   loss = 'categorical_crossentropy', \n",
    "                   metrics = ['accuracy'])\n",
    "classifier.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "600/600 [==============================] - 137s 228ms/step - loss: 1.4192 - acc: 0.4095 - val_loss: 1.2511 - val_acc: 0.5312\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.25111, saving model to ./output_models/Model01_0.53.h5\n",
      "Epoch 2/1000\n",
      "600/600 [==============================] - 106s 177ms/step - loss: 1.1907 - acc: 0.5392 - val_loss: 1.0506 - val_acc: 0.6115\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.25111 to 1.05064, saving model to ./output_models/Model02_0.61.h5\n",
      "Epoch 3/1000\n",
      "600/600 [==============================] - 107s 178ms/step - loss: 1.0356 - acc: 0.6045 - val_loss: 0.9286 - val_acc: 0.6594\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.05064 to 0.92857, saving model to ./output_models/Model03_0.66.h5\n",
      "Epoch 4/1000\n",
      "600/600 [==============================] - 106s 177ms/step - loss: 0.9597 - acc: 0.6375 - val_loss: 0.8154 - val_acc: 0.7240\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.92857 to 0.81544, saving model to ./output_models/Model04_0.72.h5\n",
      "Epoch 5/1000\n",
      "600/600 [==============================] - 106s 177ms/step - loss: 0.9054 - acc: 0.6606 - val_loss: 0.8001 - val_acc: 0.7042\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.81544 to 0.80011, saving model to ./output_models/Model05_0.70.h5\n",
      "Epoch 6/1000\n",
      "600/600 [==============================] - 107s 178ms/step - loss: 0.8514 - acc: 0.6840 - val_loss: 0.7076 - val_acc: 0.7458\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.80011 to 0.70764, saving model to ./output_models/Model06_0.75.h5\n",
      "Epoch 7/1000\n",
      "600/600 [==============================] - 108s 180ms/step - loss: 0.8272 - acc: 0.6922 - val_loss: 0.7248 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/1000\n",
      "600/600 [==============================] - 110s 183ms/step - loss: 0.7969 - acc: 0.7049 - val_loss: 0.6746 - val_acc: 0.7583\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.70764 to 0.67455, saving model to ./output_models/Model08_0.76.h5\n",
      "Epoch 9/1000\n",
      "600/600 [==============================] - 115s 192ms/step - loss: 0.7733 - acc: 0.7133 - val_loss: 0.6258 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.67455 to 0.62583, saving model to ./output_models/Model09_0.78.h5\n",
      "Epoch 10/1000\n",
      "600/600 [==============================] - 110s 183ms/step - loss: 0.7558 - acc: 0.7150 - val_loss: 0.6677 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/1000\n",
      "600/600 [==============================] - 107s 179ms/step - loss: 0.7341 - acc: 0.7292 - val_loss: 0.6609 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/1000\n",
      "600/600 [==============================] - 112s 187ms/step - loss: 0.7155 - acc: 0.7375 - val_loss: 0.6187 - val_acc: 0.7771\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.62583 to 0.61873, saving model to ./output_models/Model12_0.78.h5\n",
      "Epoch 13/1000\n",
      "600/600 [==============================] - 110s 183ms/step - loss: 0.7025 - acc: 0.7403 - val_loss: 0.6539 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/1000\n",
      "600/600 [==============================] - 111s 186ms/step - loss: 0.7030 - acc: 0.7406 - val_loss: 0.6013 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.61873 to 0.60130, saving model to ./output_models/Model14_0.79.h5\n",
      "Epoch 15/1000\n",
      "600/600 [==============================] - 109s 181ms/step - loss: 0.6840 - acc: 0.7477 - val_loss: 0.5863 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.60130 to 0.58629, saving model to ./output_models/Model15_0.78.h5\n",
      "Epoch 16/1000\n",
      "600/600 [==============================] - 107s 179ms/step - loss: 0.6698 - acc: 0.7523 - val_loss: 0.6107 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/1000\n",
      "600/600 [==============================] - 114s 191ms/step - loss: 0.6715 - acc: 0.7521 - val_loss: 0.6275 - val_acc: 0.7688\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/1000\n",
      "600/600 [==============================] - 115s 192ms/step - loss: 0.6636 - acc: 0.7582 - val_loss: 0.5792 - val_acc: 0.7781\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.58629 to 0.57917, saving model to ./output_models/Model18_0.78.h5\n",
      "Epoch 19/1000\n",
      "600/600 [==============================] - 107s 179ms/step - loss: 0.6524 - acc: 0.7586 - val_loss: 0.5787 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.57917 to 0.57870, saving model to ./output_models/Model19_0.79.h5\n",
      "Epoch 20/1000\n",
      "600/600 [==============================] - 115s 191ms/step - loss: 0.6321 - acc: 0.7665 - val_loss: 0.5727 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.57870 to 0.57273, saving model to ./output_models/Model20_0.80.h5\n",
      "Epoch 21/1000\n",
      "600/600 [==============================] - 117s 195ms/step - loss: 0.6344 - acc: 0.7625 - val_loss: 0.5540 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.57273 to 0.55400, saving model to ./output_models/Model21_0.80.h5\n",
      "Epoch 22/1000\n",
      "600/600 [==============================] - 108s 180ms/step - loss: 0.6279 - acc: 0.7706 - val_loss: 0.5525 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.55400 to 0.55245, saving model to ./output_models/Model22_0.80.h5\n",
      "Epoch 23/1000\n",
      "600/600 [==============================] - 110s 183ms/step - loss: 0.6244 - acc: 0.7716 - val_loss: 0.5913 - val_acc: 0.7865\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/1000\n",
      "600/600 [==============================] - 115s 192ms/step - loss: 0.6097 - acc: 0.7789 - val_loss: 0.5529 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/1000\n",
      "600/600 [==============================] - 128s 213ms/step - loss: 0.6170 - acc: 0.7739 - val_loss: 0.5488 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.55245 to 0.54878, saving model to ./output_models/Model25_0.79.h5\n",
      "Epoch 26/1000\n",
      "600/600 [==============================] - 122s 203ms/step - loss: 0.5984 - acc: 0.7807 - val_loss: 0.5610 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/1000\n",
      "600/600 [==============================] - 110s 184ms/step - loss: 0.5946 - acc: 0.7811 - val_loss: 0.5562 - val_acc: 0.7865\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/1000\n",
      "600/600 [==============================] - 109s 182ms/step - loss: 0.5952 - acc: 0.7813 - val_loss: 0.5481 - val_acc: 0.8104\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.54878 to 0.54812, saving model to ./output_models/Model28_0.81.h5\n",
      "Epoch 29/1000\n",
      "600/600 [==============================] - 107s 178ms/step - loss: 0.5867 - acc: 0.7855 - val_loss: 0.5177 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.54812 to 0.51768, saving model to ./output_models/Model29_0.82.h5\n",
      "Epoch 30/1000\n",
      "600/600 [==============================] - 110s 184ms/step - loss: 0.5784 - acc: 0.7895 - val_loss: 0.5422 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/1000\n",
      "600/600 [==============================] - 124s 207ms/step - loss: 0.5797 - acc: 0.7894 - val_loss: 0.5350 - val_acc: 0.8177\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/1000\n",
      "600/600 [==============================] - 107s 178ms/step - loss: 0.5813 - acc: 0.7882 - val_loss: 0.5231 - val_acc: 0.8135\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/1000\n",
      "600/600 [==============================] - 109s 181ms/step - loss: 0.5639 - acc: 0.7933 - val_loss: 0.5259 - val_acc: 0.8177\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/1000\n",
      "600/600 [==============================] - 112s 187ms/step - loss: 0.5637 - acc: 0.7932 - val_loss: 0.5243 - val_acc: 0.8146\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 00034: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f294143bf60>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "tensorboard=TensorBoard(log_dir='./logs/', histogram_freq=0, \n",
    "                         batch_size=32, write_graph=True, \n",
    "                         write_grads=True, write_images=True)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./output_models/Model{epoch:02d}_{val_acc:.2f}.h5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, \n",
    "                               patience=5, verbose=1, mode='auto')\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 600,\n",
    "                         epochs = 1000,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 30,\n",
    "                         callbacks =[tensorboard,early_stopping,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.save('./output_models/cnn_82_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = load_model('./output_models/cnn_867.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3753 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "final_test_set = test_datagen.flow_from_directory(folder_path+\"test/\",\n",
    "                                            shuffle=False,\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "y_pred = classifier.predict_generator(final_test_set)\n",
    "y_pred = np.argmax(y_pred,axis = 1) \n",
    "y_actual = final_test_set.classes\n",
    "\n",
    "classnames=[]\n",
    "for classname in final_test_set.class_indices:\n",
    "    classnames.append(classname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[667  40  27  41  20]\n",
      " [ 25 498  25  22   8]\n",
      " [ 29  36 390  12 107]\n",
      " [ 17  10   4 893   7]\n",
      " [ 15  25  35  11 789]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           BabyHat       0.89      0.84      0.86       795\n",
      "         BabyPants       0.82      0.86      0.84       578\n",
      "         BabyShirt       0.81      0.68      0.74       574\n",
      "  womencasualshoes       0.91      0.96      0.94       931\n",
      "womenlongsleevetop       0.85      0.90      0.87       875\n",
      "\n",
      "       avg / total       0.86      0.86      0.86      3753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_mtx = confusion_matrix(y_actual, y_pred) \n",
    "print(confusion_mtx)\n",
    "target_names = classnames\n",
    "print(classification_report(y_actual, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
